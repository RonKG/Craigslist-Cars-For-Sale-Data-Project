{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Craigslist Cars For Sale Data Project Part 1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV_yvxGiX0Gt"
      },
      "source": [
        ""
      ],
      "id": "PV_yvxGiX0Gt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5NcSzulF1I6"
      },
      "source": [
        "##\n",
        "#1 - go to car postings pages\n",
        "#2 - for each page, go to the url for each post\n",
        "#3 - on each car page, grab all the car attributes on the bottom left quadrant\n",
        "#4 - append to dataframe\n",
        "#5 - perform cleaning steps\n",
        "#6 - visualize\n",
        "##"
      ],
      "id": "C5NcSzulF1I6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2055KuKhF1JD"
      },
      "source": [
        "# <font color='red'> Import libraries </font>"
      ],
      "id": "2055KuKhF1JD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxDudMFGF1JJ"
      },
      "source": [
        "# work with dataframes\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# we'll use this to store the a dataframe to csv at a later stage\n",
        "import csv\n",
        "\n",
        "# make HTTP requests to a specified URL\n",
        "import requests\n",
        "\n",
        "# web scraping library\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# time management\n",
        "import time\n",
        "from random import randint\n",
        "# from collections import Counter\n",
        "\n",
        "# regular expressions\n",
        "import re"
      ],
      "id": "nxDudMFGF1JJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPEk9bNtDuLl"
      },
      "source": [
        "# Visualization Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "%matplotlib inline\n",
        "\n",
        "# Set the default color cycle for charts\n",
        "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=[\"r\", \"#e94cdc\", \"0.7\"]) "
      ],
      "id": "QPEk9bNtDuLl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntXv0hWtNrRX",
        "outputId": "f4a5f2be-a838-4c3a-9b5b-9a737752c1f0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "id": "ntXv0hWtNrRX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3nb2jTkF1JL"
      },
      "source": [
        "#### Use BeautifulSoup to scrape Craigslist DFW Used Cars"
      ],
      "id": "l3nb2jTkF1JL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJAVvL-zF1JN",
        "outputId": "8a1d3fc7-7467-422d-e9f5-23a1c77ccb54"
      },
      "source": [
        "# a link to each vehicle post will be stored here\n",
        "links = []\n",
        "\n",
        "#  all pages in a city will be stored in this list\n",
        "list_of_cities = []\n",
        "\n",
        "# list of city names we want to get data for\n",
        "cities = ['dallas','chicago', 'newyork', 'sfbay', 'losangeles', \\\n",
        "        'houston', 'phoenix', 'philadelphia', 'sanantonio', 'washingtondc',\\\n",
        "       'boston', 'nashville', 'atlanta', 'miami', 'seattle']\n",
        "\n",
        "\n",
        "for city in cities:\n",
        "    # each city has approxiamtely 1800 pages for the \"cars for sale by owner\" category\n",
        "    # we'll keep track of these pages with page_number variable below\n",
        "    page_number = 1\n",
        "\n",
        "    # this while loop cycles through all 1800 pages\n",
        "    while page_number <= 1800:\n",
        "        # city_link variable takes a a different city name from the cities every time through the loop\n",
        "        city_link = \"https://\" + str(city) + \".craigslist.org/d/cars-trucks-by-owner/search/cto?s=\" + \\\n",
        "                                str(page_number) + \"&hasPic=1\"\n",
        "\n",
        "        # we                         \n",
        "        list_of_cities.append(city_link)\n",
        "        page_number +=120\n",
        "\n",
        "# URLs counter\n",
        "car_urls = 1      \n",
        "for each_city_page in list_of_cities:\n",
        "    links_in_each_city_page = requests.get(each_city_page)\n",
        "    # parse html object from page to BS object\n",
        "    soup = BeautifulSoup(links_in_each_city_page.content, 'html.parser')\n",
        "\n",
        "    try:\n",
        "        #get the macro-container for the car posts for that page\n",
        "        posts = soup.find_all('a', class_= 'result-image gallery')\n",
        "\n",
        "        # get all the html links in the page and append them to a list\n",
        "        for link in posts:\n",
        "            l = link.get('href')\n",
        "            links.append(l)\n",
        "                \n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # this code just helps us keep count of the looping progress                     \n",
        "    if car_urls % 5 == 0:\n",
        "        city_link = l.strip()\n",
        "        start = city_link.find(\"//\") + len(\"//\")\n",
        "        end = city_link.find(\".\")\n",
        "        city_string = city_link[start:end]\n",
        "        print('Number of pages returned --> ' + str(car_urls) + '---' + city_string)\n",
        "\n",
        "    # we add a sleep timer to manage our server requests\n",
        "    time.sleep(randint(0,1))\n",
        "    car_urls +=1"
      ],
      "id": "sJAVvL-zF1JN",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of pages returned --> 5---dallas\n",
            "Number of pages returned --> 10---dallas\n",
            "Number of pages returned --> 15---dallas\n",
            "Number of pages returned --> 20---chicago\n",
            "Number of pages returned --> 25---chicago\n",
            "Number of pages returned --> 30---chicago\n",
            "Number of pages returned --> 35---newyork\n",
            "Number of pages returned --> 40---newyork\n",
            "Number of pages returned --> 45---newyork\n",
            "Number of pages returned --> 50---sfbay\n",
            "Number of pages returned --> 55---sfbay\n",
            "Number of pages returned --> 60---sfbay\n",
            "Number of pages returned --> 65---losangeles\n",
            "Number of pages returned --> 70---losangeles\n",
            "Number of pages returned --> 80---houston\n",
            "Number of pages returned --> 85---houston\n",
            "Number of pages returned --> 90---houston\n",
            "Number of pages returned --> 95---phoenix\n",
            "Number of pages returned --> 100---phoenix\n",
            "Number of pages returned --> 105---phoenix\n",
            "Number of pages returned --> 110---philadelphia\n",
            "Number of pages returned --> 115---philadelphia\n",
            "Number of pages returned --> 120---philadelphia\n",
            "Number of pages returned --> 125---sanantonio\n",
            "Number of pages returned --> 130---sanantonio\n",
            "Number of pages returned --> 135---sanantonio\n",
            "Number of pages returned --> 140---washingtondc\n",
            "Number of pages returned --> 145---washingtondc\n",
            "Number of pages returned --> 150---washingtondc\n",
            "Number of pages returned --> 155---boston\n",
            "Number of pages returned --> 160---boston\n",
            "Number of pages returned --> 165---boston\n",
            "Number of pages returned --> 170---nashville\n",
            "Number of pages returned --> 175---nashville\n",
            "Number of pages returned --> 180---nashville\n",
            "Number of pages returned --> 185---atlanta\n",
            "Number of pages returned --> 190---atlanta\n",
            "Number of pages returned --> 195---atlanta\n",
            "Number of pages returned --> 200---miami\n",
            "Number of pages returned --> 205---miami\n",
            "Number of pages returned --> 210---miami\n",
            "Number of pages returned --> 215---seattle\n",
            "Number of pages returned --> 220---seattle\n",
            "Number of pages returned --> 225---seattle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IUkfoUWF1JR"
      },
      "source": [
        "# save links to csv\n",
        "df = pd.DataFrame(links)\n",
        "df.to_csv(\"./links.csv\", sep=',',index=False)"
      ],
      "id": "2IUkfoUWF1JR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT7m6OlnOU__",
        "outputId": "3f3a568d-8df0-41a2-a4b9-e852672873ba"
      },
      "source": [
        "# read links from csv\n",
        "links = pd.read_csv('./links.csv',names=['https'])\n",
        "links = links['https'][1:]\n",
        "print(\"links returned --> \" , len(links))\n",
        "links"
      ],
      "id": "FT7m6OlnOU__",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "links returned -->  225\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1      https://dallas.craigslist.org/d/cars-trucks-by...\n",
              "2      https://dallas.craigslist.org/d/cars-trucks-by...\n",
              "3      https://dallas.craigslist.org/d/cars-trucks-by...\n",
              "4      https://dallas.craigslist.org/d/cars-trucks-by...\n",
              "5      https://dallas.craigslist.org/d/cars-trucks-by...\n",
              "                             ...                        \n",
              "221    https://Seattle.craigslist.org/d/cars-trucks-b...\n",
              "222    https://Seattle.craigslist.org/d/cars-trucks-b...\n",
              "223    https://Seattle.craigslist.org/d/cars-trucks-b...\n",
              "224    https://Seattle.craigslist.org/d/cars-trucks-b...\n",
              "225    https://Seattle.craigslist.org/d/cars-trucks-b...\n",
              "Name: https, Length: 225, dtype: object"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxPI82ppF1Jc"
      },
      "source": [
        "# justa counter\n",
        "count = 0\n",
        "\n",
        "# store vehicle details in this list\n",
        "cars = []\n",
        "\n",
        "#loop over all links in the list    \n",
        "for link in links:\n",
        "    # make HTTP requests\n",
        "    each_page = requests.get(link)\n",
        "    # The sleep function can help you to avoid the server to be overloaded with too many requests in a very short period of time.\n",
        "    time.sleep(randint(1,2))\n",
        "    # store the BS object in a variable\n",
        "    page_soup = BeautifulSoup(each_page.content, 'html.parser')\n",
        "\n",
        "    # loop over each link and store car details\n",
        "    car_details = []\n",
        "    try:\n",
        "        # find price attribute and store in car details\n",
        "        car_details.append(page_soup.find('span', class_=\"price\").text)\n",
        "\n",
        "        # find date time and append to car details\n",
        "        for span in page_soup.find_all('span', recursive=True):\n",
        "            if not span.attrs.values():\n",
        "                car_details.append(span.text)\n",
        "        car_details.append(\"date time: \" + page_soup.find('time', class_=\"date timeago\")\\\n",
        "                                         .text.strip().replace(':',';'))\n",
        "\n",
        "        # find date city name and append to car details\n",
        "        city = link.strip()\n",
        "        start = city.find(\"//\") + len(\"//\")\n",
        "        end = city.find(\".\")\n",
        "        substring = city[start:end]\n",
        "        car_details.append('city:' + substring)\n",
        "\n",
        "        # find geo coordinates and append to car details\n",
        "        geos = page_soup.findAll(\"div\", {\"class\": \"mapbox\"})\n",
        "        lat = geos[0].contents[1].get('data-latitude')\n",
        "        car_details.append('lat:' + lat.strip())\n",
        "        long = geos[0].contents[1].get('data-longitude')\n",
        "        car_details.append('long:' + long.strip())\n",
        "\n",
        "        # find post body and append to car details\n",
        "        post_body = page_soup.find(attrs={'id' : 'postingbody'}).contents[2]\n",
        "        # remove non ascii characters from post bosy\n",
        "        car_details.append('post_body:' + re.sub(\"[^0-9a-zA-Z]+\", \" \", post_body))\n",
        "\n",
        "        # find postID and append to car details / We'll use this to assign labels to images later\n",
        "        car_details.append('pID:' + link.strip().replace('html','').replace('.','').split('/')[-1])\n",
        "    except:\n",
        "        pass    \n",
        "    \n",
        "    # perform some basic cleanup and store in clean\n",
        "    clean = []\n",
        "    for string in car_details:\n",
        "        # this attribute came without a label. Assign one.\n",
        "        if string == car_details[1]:\n",
        "            clean.append('year make model: ' + string)\n",
        "        # clean up price text from $9,999 --> 9999\n",
        "        if string == car_details[0]:\n",
        "            clean.append('price: ' + string.replace(',','').replace('$',''))\n",
        "        else:\n",
        "            clean.append(string)\n",
        "\n",
        "            \n",
        "    # some attributes came without labels. Drop those.\n",
        "    car_final = []\n",
        "    for s in clean:\n",
        "        if ':' in s:\n",
        "            car_final.append(s)\n",
        "            \n",
        "    # append clean attributes for each vehicle to car list\n",
        "    cars.append(car_final)\n",
        "    count += 1\n",
        "\n",
        "    # just a counter to keep track of the loop\n",
        "    if count % 100 == 0:\n",
        "        print('loop # -> ',count)"
      ],
      "id": "MxPI82ppF1Jc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eweb1lJF1Jg"
      },
      "source": [
        "# method to strip() the keys and values after splitting in order to trim white-space.\n",
        "def list_to_dict(rlist):\n",
        "    return dict(map(lambda s : map(str.strip, s.split(':')), rlist))\n",
        "\n",
        "\n",
        "\n",
        "# create a dictionary for label:value for each car attribute\n",
        "car_dicts = []\n",
        "for car in cars:\n",
        "    car_dict = list_to_dict(car)\n",
        "    car_dicts.append(car_dict)"
      ],
      "id": "5eweb1lJF1Jg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE0oExKHF1Jh"
      },
      "source": [
        "# create an Empty DataFrame object\n",
        "dfs = pd.DataFrame()\n",
        "\n",
        "for item in car_dicts:\n",
        "    df = pd.DataFrame.from_dict(item,orient='index').transpose()\n",
        "    #concatenate each new df from the loop into the parent df\n",
        "    dfs= pd.concat([dfs,df], axis=0, ignore_index=True, sort=True)\n",
        "    #clean duplicate year in 'year make model'\n",
        "    dfs['year_c make model'] = dfs['year make model'].str.replace(r'\\b(\\w+)(\\s+\\1)+\\b', r'\\1', regex=True)"
      ],
      "id": "GE0oExKHF1Jh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT2BPEhKF1Jj"
      },
      "source": [
        "dfs.to_csv('car_data.csv', sep='\\t', encoding='utf-8')\n",
        "dfs.to_csv('B:/DS/car_data.csv', sep='\\t', encoding='utf-8')"
      ],
      "id": "qT2BPEhKF1Jj",
      "execution_count": null,
      "outputs": []
    }
  ]
}
